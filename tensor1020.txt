import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

# class_names = [
# "NO"
# "YES"
# ]

import pandas as pd

#df = pd.DataFrame(data)
#new_data = pd.read_csv("requirements copy.txt")
df = pd.read_csv("required77.txt")
# print(df.columns)
# print(df)


df['yesorno'] = df['yesorno'].astype(int)

features = df.iloc[:, 0]
print("Features: ")
print(features)
labels = df['yesorno']
print("Labels: ")
print(labels)
#print(df)

data = [line.strip() for line in df]
dataset = tf.data.Dataset.from_tensor_slices(data)

vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=1000, output_mode='int')  # You can adjust max_tokens
vectorize_layer.adapt(features.values)  # Fit the vectorization layer on the text data

features_vectorized = vectorize_layer(features.values)
# [[38  6 23 18 52  8 24 47 11 10 46 15 30  8  2 19 34 12 31 35 33 29 28 14
#    2 41  2 51  2 26 55 40  2 25  5 16  6 50 13 48 37 57 20 44 21 22 13 42
#   39 54 27  3  4 58 49 53 12  9 32  3  4  5  7 17 43 56 36  9  3  4  5  7
#   45 10 11]]
# features_numpy = features_vectorized.numpy()
# for i in features_numpy:
#     print(features_numpy[i].item())
# Split the data into training and test sets
split_index = int(0.8 * len(df))
X_train, X_test = features_vectorized[:split_index], features_vectorized[split_index:]
y_train, y_test = labels[:split_index], labels[split_index:]

# Create TensorFlow datasets
ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)
ds_test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)
#PRAISE THE LORD WITH ALL YOUR HEART

# Verify the batch of training data
for features, labels in ds_train.take(1):
    print("Features:", features)
    print("Labels:", labels)
#PRAISE THE LORD WITH ALL YOUR HEART

# No dataset info as with tfds.load, but you can print samples
print("Training dataset sample:")
for example in ds_train.take(1):
    print(example)

print("Test dataset sample:")
for example in ds_test.take(1):
    print(example)

model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(None,), dtype='int32'),  # Input layer for the vectorized text
    tf.keras.layers.Embedding(input_dim=1000, output_dim=64),  # Embedding layer
    tf.keras.layers.GlobalAveragePooling1D(),  # Pooling layer
    tf.keras.layers.Dense(32, activation='relu'),  # Dense hidden layer
    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])
#
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

predictions = model(features)
predictions[:5]

print("Prediction: {}".format(tf.math.argmax(predictions, axis=1)))
print("    Labels: {}".format(labels))

loss_object = tf.keras.losses.BinaryCrossentropy() 

def loss(model, x, y, training):
  # training=training is needed only if there are layers with different
  # behavior during training versus inference (e.g. Dropout).
  y_ = model(x, training=training)

  return loss_object(y_true=y, y_pred=y_)

l = loss(model, features, labels, training=False)
print("Loss test: {}".format(l))

def grad(model, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value = loss(model, inputs, targets, training=True)
  return loss_value, tape.gradient(loss_value, model.trainable_variables)

optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)

loss_value, grads = grad(model, features, labels)

print("Step: {}, Initial Loss: {}".format(optimizer.iterations.numpy(),
                                          loss_value.numpy()))

optimizer.apply_gradients(zip(grads, model.trainable_variables))

print("Step: {},         Loss: {}".format(optimizer.iterations.numpy(),
                                          loss(model, features, labels, training=True).numpy()))

train_loss_results = []
train_accuracy_results = []

num_epochs = 10001
#
for epoch in range(num_epochs):
    epoch_loss_avg = tf.keras.metrics.Mean()
    epoch_accuracy = tf.keras.metrics.BinaryAccuracy()  # Changed to BinaryAccuracy

    # Training loop - using batches of 32
    for x, y in ds_train:
        # Optimize the model
        loss_value, grads = grad(model, x, y)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        # Track progress
        epoch_loss_avg.update_state(loss_value)  # Add current batch loss
        epoch_accuracy.update_state(y, model(x, training=True))  # Update binary accuracy with predictions

    # End epoch
    train_loss_results.append(epoch_loss_avg.result())
    train_accuracy_results.append(epoch_accuracy.result())

    if epoch % 20 == 0:
        print("Epoch {:04d}: Loss: {:.5f}, Accuracy: {:.5%}".format(epoch,
                                                                    epoch_loss_avg.result(),
                                                                    epoch_accuracy.result()))
#
# Test accuracy evaluation
test_accuracy = tf.keras.metrics.BinaryAccuracy()  # Use BinaryAccuracy for binary classification

for (x, y) in ds_test:
    logits = model(x, training=False)
    prediction = tf.where(logits > 0.5, 1, 0)  # Thresholding at 0.5 for binary predictions
    test_accuracy.update_state(y, prediction)

print("Test set accuracy: {:.3%}".format(test_accuracy.result()))
#
prediction = tf.squeeze(prediction)
prediction = tf.cast(prediction, tf.int64)
print(tf.stack([y,prediction],axis=1))

model.save('my_model737.h5')
#model.save(r'C:\Users\nickc\Downloads\kerastest.h5')
# from keras.models import load_model

# # Load the saved model
# loaded_model = load_model('my_model.h5')
# text_data = new_data['connection']  # Replace 'connection' with your actual column name

# # Ensure all entries are strings (if needed)
# text_data = text_data.astype(str)

# # Vectorize the input data
# vectorized_data = vectorize_layer(text_data)

# # Make sure to reshape the data if required
# # For instance, if your model expects a 2D array with shape (n_samples, n_features)
# vectorized_data = np.array(vectorized_data)

# # If necessary, adjust the shape:
# if vectorized_data.ndim == 1:
#     vectorized_data = vectorized_data.reshape(-1, 1)  # Adjust shape if necessary

# features = new_data.iloc[:, 0]
# print("Features: ")
# print(features)
# labels = new_data['yesorno']
# print("Labels: ")
# print(labels)
# #print(df)
# # Process the lines into a TensorFlow Dataset
# data = [line.strip() for line in df]
# dataset = tf.data.Dataset.from_tensor_slices(data)

# vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=1000, output_mode='int')  # You can adjust max_tokens
# vectorize_layer.adapt(features.values)  # Fit the vectorization layer on the text data

# # Transform features into a vectorized form
# features_vectorized = vectorize_layer(features.values)

# # Split the data into training and test sets
# split_index = int(0.8 * len(df))
# X_train, X_test = features_vectorized[:split_index], features_vectorized[split_index:]
# y_train, y_test = labels[:split_index], labels[split_index:]

# # Create TensorFlow datasets
# ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)
# ds_test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)

# # Load your model
# loaded_model = tf.keras.models.load_model('my_model.keras', compile=False)

# # Make predictions
# predictions = loaded_model.predict(vectorized_data)
# predictions[:5]

# print("Prediction: {}".format(tf.math.argmax(predictions, axis=1)))
# print("    Labels: {}".format(labels))

# loss_object = tf.keras.losses.BinaryCrossentropy() 

# def loss(model, x, y, training):
#   # training=training is needed only if there are layers with different
#   # behavior during training versus inference (e.g. Dropout).
#   y_ = model(x, training=training)

#   return loss_object(y_true=y, y_pred=y_)

# l = loss(model, features, labels, training=False)
# print("Loss test: {}".format(l))

# def grad(model, inputs, targets):
#   with tf.GradientTape() as tape:
#     loss_value = loss(model, inputs, targets, training=True)
#   return loss_value, tape.gradient(loss_value, model.trainable_variables)

# optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)

# loss_value, grads = grad(model, features, labels)

# print("Step: {}, Initial Loss: {}".format(optimizer.iterations.numpy(),
#                                           loss_value.numpy()))

# optimizer.apply_gradients(zip(grads, model.trainable_variables))

# print("Step: {},         Loss: {}".format(optimizer.iterations.numpy(),
#                                           loss(model, features, labels, training=True).numpy()))

# train_loss_results = []
# train_accuracy_results = []

# num_epochs = 21

# for epoch in range(num_epochs):
#     epoch_loss_avg = tf.keras.metrics.Mean()
#     epoch_accuracy = tf.keras.metrics.BinaryAccuracy()  # Changed to BinaryAccuracy

#     # Training loop - using batches of 32
#     for x, y in ds_train:
#         # Optimize the model
#         loss_value, grads = grad(model, x, y)
#         optimizer.apply_gradients(zip(grads, model.trainable_variables))

#         # Track progress
#         epoch_loss_avg.update_state(loss_value)  # Add current batch loss
#         epoch_accuracy.update_state(y, model(x, training=True))  # Update binary accuracy with predictions

#     # End epoch
#     train_loss_results.append(epoch_loss_avg.result())
#     train_accuracy_results.append(epoch_accuracy.result())

#     if epoch % 1 == 0:
#         print("Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(epoch,
#                                                                     epoch_loss_avg.result(),
#                                                                     epoch_accuracy.result()))

# # Test accuracy evaluation
# test_accuracy = tf.keras.metrics.BinaryAccuracy()  # Use BinaryAccuracy for binary classification

# for (x, y) in ds_test:
#     logits = model(x, training=False)
#     prediction = tf.where(logits > 0.5, 1, 0)  # Thresholding at 0.5 for binary predictions
#     test_accuracy.update_state(y, prediction)

# print("Test set accuracy: {:.3%}".format(test_accuracy.result()))

# prediction = tf.squeeze(prediction)
# prediction = tf.cast(prediction, tf.int64)
# print(tf.stack([y,prediction],axis=1))


